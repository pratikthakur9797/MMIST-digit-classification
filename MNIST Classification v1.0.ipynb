{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1473c84f",
   "metadata": {},
   "source": [
    "# This is my first attempt on the MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2e67fa90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5bbc187e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset already exists in tensorflow library\n",
    "mnist_dataset, mnist_info = tfds.load(name='mnist', with_info=True, as_supervised=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8881f8e5",
   "metadata": {},
   "source": [
    "### Working with the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "288db65a",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_train, mnist_test = mnist_dataset['train'], mnist_dataset['test']\n",
    "\n",
    "#creating validation data\n",
    "num_validation_samples = 0.1 * mnist_info.splits['train'].num_examples\n",
    "num_validation_samples = tf.cast(num_validation_samples, tf.int64)\n",
    "\n",
    "num_test_samples = mnist_info.splits['test'].num_examples\n",
    "num_test_samples = tf.cast(num_test_samples, tf.int64)\n",
    "\n",
    "#creating feature scaling fucntion\n",
    "def scale(image, label):\n",
    "    image = tf.cast(image, tf.float32)\n",
    "    image /= 255.\n",
    "    return image, label\n",
    "\n",
    "#scaling train, validation, and test data\n",
    "scaled_train_and_validation_data = mnist_train.map(scale)\n",
    "scaled_test_data = mnist_test.map(scale)\n",
    "\n",
    "#shuffling the data\n",
    "buffer_size = 10000 #used only for big datasets\n",
    "shuffled_train_and_validation_data = scaled_train_and_validation_data.shuffle(buffer_size)\n",
    "\n",
    "#setting validation and train data\n",
    "validation_data = shuffled_train_and_validation_data.take(num_validation_samples)\n",
    "train_data = shuffled_train_and_validation_data.skip(num_validation_samples)\n",
    "\n",
    "#setting batch size for gradient descent\n",
    "batch_size = 100\n",
    "train_data = train_data.batch(batch_size)\n",
    "validation_data = validation_data.batch(num_validation_samples) #we don't actually need to batch it, as we are only forward propagating on validation data. However, we use the batch method as the built in methods expect the data in batch form\n",
    "test_data = scaled_test_data.batch(num_test_samples) #same logic as validation data\n",
    "\n",
    "validation_inputs, validation_targets = next(iter(validation_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35e7609d",
   "metadata": {},
   "source": [
    "### Creating the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "0d38fbd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 784 input nodes (28x28 pixels), 150 hidden nodes, 3 hidden layers, 10 output nodes (digits from 1 to 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "9b1bdb15",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 784\n",
    "output_size = 10\n",
    "hidden_layer_size = 150\n",
    "\n",
    "#creating model layers\n",
    "model = tf.keras.Sequential([\n",
    "                            tf.keras.layers.Flatten(input_shape = (28, 28, 1)),\n",
    "                            tf.keras.layers.Dense(hidden_layer_size, activation = 'relu'),\n",
    "                            tf.keras.layers.Dense(hidden_layer_size, activation = 'tanh'),\n",
    "                            tf.keras.layers.Dense(hidden_layer_size, activation = 'relu'),\n",
    "                            tf.keras.layers.Dense(output_size, activation = 'softmax') #softmax gives us output as probablity distribution\n",
    "                            ]) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be902e8b",
   "metadata": {},
   "source": [
    "### Optimizer and Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "0a168750",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer = 'adam', loss = 'sparse_categorical_crossentropy', metrics = ['accuracy']) #Note: strings are not case-sensitive. However, methods are!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2a6cd23",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "d439d169",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "540/540 - 2s - loss: 0.0422 - accuracy: 0.9867 - val_loss: 0.0479 - val_accuracy: 0.9847\n",
      "Epoch 2/5\n",
      "540/540 - 1s - loss: 0.0359 - accuracy: 0.9882 - val_loss: 0.0475 - val_accuracy: 0.9848\n",
      "Epoch 3/5\n",
      "540/540 - 1s - loss: 0.0291 - accuracy: 0.9905 - val_loss: 0.0381 - val_accuracy: 0.9878\n",
      "Epoch 4/5\n",
      "540/540 - 1s - loss: 0.0250 - accuracy: 0.9916 - val_loss: 0.0254 - val_accuracy: 0.9922\n",
      "Epoch 5/5\n",
      "540/540 - 1s - loss: 0.0231 - accuracy: 0.9921 - val_loss: 0.0250 - val_accuracy: 0.9920\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1cd2c3fcbb0>"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_epochs = 5\n",
    "\n",
    "model.fit(train_data, epochs = num_epochs, validation_data = (validation_inputs, validation_targets), verbose = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af505c47",
   "metadata": {},
   "source": [
    "#### Highest validation accuraty 99.20%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d60b7b83",
   "metadata": {},
   "source": [
    "### Testing the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "357b569a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0787 - accuracy: 0.9800\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_accuraty = model.evaluate(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eb8b522",
   "metadata": {},
   "source": [
    "## Final model accuracy is 98.00%!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Tensorflow 2.0)",
   "language": "python",
   "name": "py-tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
